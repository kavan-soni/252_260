{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NabZ2lZGEUaS"
   },
   "source": [
    "# HW3 - Part1, CMPE 252, Section 01, SPRING 2023\n",
    "\n",
    "\n",
    "## Topic: Sequential Decision Making in the tabular case. \n",
    "In this question you will solve MDP by **Value Iteration** and **Policy Iteration**\n",
    "algorithms with a known dynamics and reward, $T$ and $R$, respectively. \n",
    "\n",
    "You can work in teams, and discuss your solutions, but not to share your code with other teams.\n",
    "\n",
    "What to submit in Canvas: 1) a working notebook with the full solution, and 2) its corresponding PDF. \n",
    "\n",
    "\n",
    "Due date : April 7, 11:59PM\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "lm_n1eT3EUaU"
   },
   "source": [
    "import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XeIvwWOcEUaV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "lm_n1eT3EUaU"
   },
   "source": [
    "Define the variables needed for the Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_it = 100\n",
    "\n",
    "# Set DEF_Qx to True to solve for Task x\n",
    "DEF_Q3 = True\n",
    "DEF_Q5_1 = True\n",
    "DEF_Q5_2 = True\n",
    "DEF_Q6 = True\n",
    "DEF_Q7 = True\n",
    "\n",
    "# For Task 5, define an arbitrary point in the middle of the maze\n",
    "ss = '???' # eg: ss = (5,5)\n",
    "V_ss = []\n",
    "V_ss1 = []\n",
    "V_ss2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Task 6, define a norm function\n",
    "def norm(m):\n",
    "    sum = 0\n",
    "    for _, val in np.ndenumerate(m):\n",
    "        sum += val*val\n",
    "    return np.sqrt(sum)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gzMMg9NpQ5dD"
   },
   "outputs": [],
   "source": [
    "def build_maze(maze_file):\n",
    "    '''\n",
    "    para1: filename of the maze txt file\n",
    "    return mazes as a numpy array walls: 0 - no wall, 1 - wall\n",
    "    '''\n",
    "    a = open(maze_file, 'r')  \n",
    "    m=[]\n",
    "    for i in a.readlines():\n",
    "        m.append(np.array(i.split(\" \"), dtype=\"int32\"))\n",
    "    return np.array(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a maze file with 0 - no wall, 1 - wall. Verify that there are multiple paths (at least 3) from the START = 'top left corner' to GOAL='bottom right corner'. If you do not have at least 3 paths, update your maze accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "jLFgpnM_EUaW",
    "outputId": "60161d66-2730-4b9b-a534-142462092174"
   },
   "outputs": [],
   "source": [
    "# State Space\n",
    "S=build_maze(****file_name***)\n",
    "START = (1,1)\n",
    "GOAL = (19,19)\n",
    "# Action Space\n",
    "A = [\n",
    "    (-1, 0),    # 'up'\n",
    "    (1, 0),     # 'down'\n",
    "    (0, -1),    # 'left'\n",
    "    (0, 1),     # 'right'\n",
    "    (0, 0)      # 'stay'\n",
    "]\n",
    "\n",
    "\n",
    "# Noise\n",
    "ALPHA = [0.2, 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoVFsScnEUaW"
   },
   "outputs": [],
   "source": [
    "\n",
    "GRID_SIZE = len(S)\n",
    "# goal state\n",
    "S[GOAL]    = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8wrecEQEUaX"
   },
   "source": [
    "Visualize the maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TDXiKR5eEUaX"
   },
   "outputs": [],
   "source": [
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R_xMFadEUaY"
   },
   "outputs": [],
   "source": [
    "plt.imshow(S, cmap='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD7ZHnSZEUaY"
   },
   "source": [
    "We will define a utility function s_next_calc which computes the index of the next state given current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFRfCFu1EUaZ"
   },
   "outputs": [],
   "source": [
    "def s_next_calc(s, a):\n",
    "    '''This function returns the agent's next state given action\n",
    "    and current state (assuming the action succeeds).\n",
    "    : param s: Current position of the agent\n",
    "    : param a: action taken by the agent\n",
    "    : returns: New state coordinates in the grid\n",
    "    '''\n",
    "    \n",
    "    return (s[0] + A[a][0], s[1] + A[a][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um83WW0EEUaZ"
   },
   "source": [
    "A Utility function to check if the action at current state leads to a collision with a wall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGOTonn_EUaZ"
   },
   "outputs": [],
   "source": [
    "def hit_wall(curr, action):\n",
    "    '''This function checks if the agent hits any walls.\n",
    "    : param curr: Current position of the agent\n",
    "    : param action: Chosen action by the agent\n",
    "    : returns: True/False Binary value to indicate if agent hits a wall\n",
    "    '''\n",
    "    s_new = (\n",
    "        curr[0] + A[action][0],\n",
    "        curr[1] + A[action][1]\n",
    "    )\n",
    "\n",
    "    # Check for grid boundaries\n",
    "    if min(s_new) < 0 or max(s_new) > GRID_SIZE-1:\n",
    "        return True\n",
    "\n",
    "    # Check walls\n",
    "    # 0: 'up':   (-1,  0),\n",
    "    # 1: 'down': ( 1,  0),\n",
    "    # 2: 'left': ( 0, -1),\n",
    "    # 3: 'right':( 0,  1),\n",
    "    # 4: 'stay': ( 0,  0)\n",
    "    if (S[curr]==0 and S[s_new]==1):\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieW0NgQmEUaa"
   },
   "source": [
    "Reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9dZ8GZkiEUaa"
   },
   "outputs": [],
   "source": [
    "def R(s, a):\n",
    "    '''Reward function\n",
    "    : param s: Current state of the agent\n",
    "    : param a: Action the agent takes at the current state\n",
    "    : returns: Reward for the action at current state\n",
    "    '''\n",
    "    if s == GOAL:\n",
    "        return 0\n",
    "    elif hit_wall(s, a):\n",
    "        return -10000\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knmSD8o_EUaa"
   },
   "source": [
    "Calculate the transition probabilities to state s_next from current state s upon action a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c3wdRpJWEUaa"
   },
   "outputs": [],
   "source": [
    "def Pr(s_next, s, a, alpha):\n",
    "    '''This function returns probabilities for each action at agent's\n",
    "    current state.\n",
    "    :param s: Current state of the agent\n",
    "    :param a: Action the agent takes at the current state\n",
    "    :param alpha: Probability of the agent to take a random action instead of the action a\n",
    "    :returns : Transition probability for the action at current state\n",
    "    '''\n",
    "    # can't move once reached the GOAL\n",
    "    if s == GOAL:\n",
    "        if s_next == s:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # If wall hit, next state is current state\n",
    "    if hit_wall(s, a):\n",
    "        # Illegal action with s = s'\n",
    "        if s_next == s:\n",
    "            return 1\n",
    "        # Illegal action with s != s'\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Legal action with adjacent s and s'\n",
    "    if s_next == s_next_calc(s, a):\n",
    "        return 1 - alpha\n",
    "    else:\n",
    "        # Agent moves to another adjacent state instead of s' due to noise in a\n",
    "        # Generate all other neighbors of s by applying actions other than a\n",
    "        other_s_next = [s_next_calc(s, i)\n",
    "                    for i in range(len(A)) if i is not a]\n",
    "        if s_next in other_s_next:\n",
    "            return alpha/4\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVgNRD13EUaa"
   },
   "source": [
    "## Policy Iteration\n",
    "\n",
    "In policy iteration, we define three functions:\n",
    "\n",
    "* policy_evaluation\n",
    "* policy_improvement\n",
    "* policy_iteration\n",
    "\n",
    "Refer to the lecture slides on policy iteration and value iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "id": "1-Kv6DAFEUab",
    "outputId": "83097735-f74e-46c6-fdd8-01d747a6a349",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, S, Pr, alpha, discount, theta, ctr):\n",
    "    \n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    \n",
    "    global V_ss # For Task 5\n",
    "    V_ss.clear() # For Task 5\n",
    " \n",
    "    for _ in range(ctr):\n",
    "        # chose an initial delta value for the convergence test (see lines 14 and 16 below) \n",
    "        delta = '???'\n",
    "        V_prev = V.copy()\n",
    "        for s, _ in np.ndenumerate(S):\n",
    "                 \n",
    "            # action by the policy\n",
    "            a = '???'\n",
    "            # update value function for the state s  \n",
    "            V[s] = '???'\n",
    "            # convergece test  \n",
    "            delta = max(delta, abs(V[s] - V_prev[s]))\n",
    "            \n",
    "            if(DEF_Q5_1 == True): # For Task 5\n",
    "                if(s == ss):\n",
    "                    V_ss.append(V[s]) \n",
    "                            \n",
    "    \n",
    "        if delta < theta: break\n",
    "    #print(delta)\n",
    "    \n",
    "    return V, delta\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCJav5gtEUab"
   },
   "outputs": [],
   "source": [
    "def policy_improvement(V, S, A, Pr, alpha, discount):\n",
    "    policy = np.zeros((GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "    policy_stable = True\n",
    "    \n",
    "    global V_ss2 # For Task 5\n",
    "        \n",
    "    for s, _ in np.ndenumerate(S):\n",
    "              \n",
    "        old_action = policy[s]\n",
    "        Q = np.zeros(len(A))\n",
    "        for a in range(len(A)):\n",
    "            # update Q function at state, s, and action, a\n",
    "            Q[a] = '???'\n",
    "            # update policy at state s\n",
    "            policy[s] = '???'\n",
    "            \n",
    "        if(DEF_Q5_2 == True): # For Task 5\n",
    "            if (s == ss):\n",
    "                V_ss2.append(max(Q)) \n",
    "            \n",
    "        if old_action != policy[s]: policy_stable = False\n",
    "    \n",
    "    return policy, policy_stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v87MzWcIEUab"
   },
   "outputs": [],
   "source": [
    "def policy_iteration(S, A, Pr, alpha, discount, theta, n_eval, plot_enable, plot=None):\n",
    "    \"\"\"\n",
    "    :param list S: set of states\n",
    "    :param list A: set of actions\n",
    "    :param function Pr: transition function\n",
    "    :param float alpha: discount factor\n",
    "    :param int n_iter: number of iterations\n",
    "    :param int n_eval: number of evaluations\n",
    "    :param plot: list of iteration numbers to plot\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 0\n",
    "    \n",
    "    # For Task 4\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    time = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    # For Task 5\n",
    "    global V_ss\n",
    "    global V_ss1\n",
    "    global V_ss2\n",
    "    V_ss.clear()\n",
    "    V_ss1.clear()\n",
    "    V_ss2.clear()\n",
    "    \n",
    "    normV = [] # For Task 6\n",
    "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE)) # For Task 6\n",
    "    deltaV = [] # For Task 7\n",
    "    \n",
    "    plt.ion()\n",
    "    policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE))\n",
    "    count=0\n",
    "    \n",
    "\n",
    "    while True:\n",
    "                \n",
    "        if(DEF_Q5_1 == True): # For Task 5\n",
    "            if(count == 10): # The iteration number of the 'policy improvement' is taken as 10. Feel free to change it.\n",
    "                V_ss1 = V_ss.copy()\n",
    "        \n",
    "        if DEF_Q3 == True: # For Task 3\n",
    "            start_time = datetime.datetime.now()\n",
    "        \n",
    "\n",
    "        V, delta = policy_evaluation(policy, S, Pr, alpha, discount, theta, n_eval)\n",
    "        policy , policy_stable = policy_improvement(V, S, A, Pr, alpha, discount)\n",
    "\n",
    "        \n",
    "        if DEF_Q3 == True: # For Task 3\n",
    "            end_time = datetime.datetime.now()\n",
    "            time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
    "            total_time += time\n",
    "      \n",
    "        if DEF_Q6 == True: # For Task 6\n",
    "            normV.append(norm(V - V_prev))\n",
    "            V_prev = V.copy()\n",
    "            \n",
    "        if DEF_Q7 == True: # For Task 7\n",
    "            deltaV.append(delta)\n",
    "            \n",
    "        \n",
    "        # plot intermediate result\n",
    "        if (plot and count+1 in plot) and plot_enable:\n",
    "            plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "        \n",
    "        if delta == 0:\n",
    "            if plot_enable :\n",
    "                plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            break\n",
    "                \n",
    "        if (delta > 0 and delta<=epsilon) or count==max_it or policy_stable:    \n",
    "            if(count == max_it):\n",
    "                print('Policy iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            if(plot_enable):\n",
    "                 plot_value_grid(V, policy, msg='Policy iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            break\n",
    "        count=count+1\n",
    "\n",
    "    return V, policy, count, total_time, normV, deltaV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_feJtvmWEUab"
   },
   "source": [
    "## Value Iteration\n",
    "\n",
    "We use the following function for value iteration. See slides starting from 161."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C74aSbLaEUab"
   },
   "outputs": [],
   "source": [
    "def value_iteration(S, A, Pr, alpha, discount, plot_enable, plot=None):\n",
    "    \"\"\"\n",
    "    :param list S: set of states\n",
    "    :param list A: set of actions\n",
    "    :param function Pr: transition function\n",
    "    :param float alpha: noise parameter\n",
    "    \"\"\"\n",
    "    # For Task 4\n",
    "    start_time = 0\n",
    "    end_time = 0\n",
    "    time = 0\n",
    "    total_time = 0\n",
    "    \n",
    "    normV = [] # For Task 6\n",
    "     \n",
    "    plt.ion()\n",
    "\n",
    "    V = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    V_prev = np.zeros((GRID_SIZE, GRID_SIZE))\n",
    "    optimal_policy = np.random.randint(0, len(A), (GRID_SIZE, GRID_SIZE), dtype=int)\n",
    "\n",
    "    count=0\n",
    "    while True:\n",
    "        \n",
    "        if DEF_Q3 == True: # For Task 3\n",
    "            start_time = datetime.datetime.now()\n",
    "    \n",
    "        delta = 0 \n",
    "        V_prev = V.copy()\n",
    "        \n",
    "        for s,_ in np.ndenumerate(S):\n",
    "            Q = np.zeros(len(A))\n",
    "            for a in range(len(A)):\n",
    "                Q[a] = '???'  # expression for the Q function at state, s, and action, a\n",
    "            V[s] = '???'\n",
    "            delta = max(delta, '???') # set the validation condition for the convergence\n",
    "            #print(delta)\n",
    "            \n",
    "            optimal_policy[s] = np.argmax(Q)\n",
    "        \n",
    "        if DEF_Q3 == True: # For Task 3\n",
    "            end_time = datetime.datetime.now()\n",
    "            time = (end_time - start_time).total_seconds() # calculate time taken [seconds] for one iteration\n",
    "            total_time += time\n",
    "        \n",
    "        if DEF_Q6 == True: # For Task 6\n",
    "            normV.append(norm(V - V_prev))\n",
    "\n",
    "        # plot current value and optimal policy\n",
    "        if (plot and count+1 in plot) and plot_enable:\n",
    "            plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            plt.pause(0.1)\n",
    "        if delta <= 0 or count==max_it: \n",
    "            if count == max_it:\n",
    "                print('Value iteration failed to converge for alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "            if(plot_enable):\n",
    "                plot_value_grid(V, optimal_policy, msg='Value iteration, alpha = {}, discount = {}, iteration = {}'.format(alpha, discount, count+1))\n",
    "                plt.pause(0.1)\n",
    "            break\n",
    "        count=count+1\n",
    "    return V, optimal_policy, count, total_time, normV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vyld48sUEUab"
   },
   "source": [
    "We will use the following utility function to plot the grid with values from V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vx-euSGWEUab"
   },
   "outputs": [],
   "source": [
    "def plot_value_grid(V, policy, msg=\"\"):\n",
    "    fig = plt.figure(figsize=(5, 5), tight_layout=True)\n",
    "    plt.title(msg)\n",
    "    plt.imshow(V)\n",
    "   \n",
    "    quiver_action_dict = [\n",
    "            [1, 0],\n",
    "            [-1, 0],\n",
    "            [0, -1],\n",
    "            [0, 1],\n",
    "            [0, 0]\n",
    "    ]\n",
    "    for k, a in np.ndenumerate(policy):\n",
    "        if(S[k] == 0): # do not print the arrows on walls, to increase readibility\n",
    "            q = plt.quiver(k[1], k[0], quiver_action_dict[a][1], quiver_action_dict[a][0])\n",
    "    #fig.colorbar(q)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fX17TgExEUab"
   },
   "source": [
    "## Tasks\n",
    "### A. Find the optimal solution by two methods for $\\alpha$ = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUGanpXzEUab"
   },
   "source": [
    "### I. Policy Iteration \n",
    "\n",
    "We are using iterative policy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZL7XN7iEUac"
   },
   "outputs": [],
   "source": [
    "n_pol_eval = 100 #number of policy evalutions\n",
    "val2, pol2, pol_max_iter, time_taken, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=1, theta = 1e-6, n_eval=n_pol_eval, plot_enable = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBac_v5gEUac"
   },
   "source": [
    "### II. Value Iteration\n",
    "\n",
    "Run for 100 iterations and no noise\n",
    "Plot the value function and the optimal policy every 20 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-diPfkcFEUac"
   },
   "outputs": [],
   "source": [
    "\n",
    "val1, pol1, val_max_iter, time_taken, normV = value_iteration(S, A, Pr, alpha=0, discount=1, plot_enable=True, plot=[1, 25, 50, 100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55aV-hYiEUac"
   },
   "source": [
    "Let's visualize these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usH6kFwPEUac"
   },
   "outputs": [],
   "source": [
    "# Plot for policy iteration alpha = 0, k_max = 100, run for 100 iterations\n",
    "plot_value_grid(val2, pol2, msg='Policy Iteration, {} iterations, {} evaluations'.format(pol_max_iter, n_pol_eval))\n",
    "# Plot for value iteration alpha = 0, 100 iterations\n",
    "plot_value_grid(val1, pol1, msg='Value Iteration, {} iterations'.format(val_max_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlBjcTl5maO9"
   },
   "source": [
    "### III. Run Policy Iteration and Value Iteration for  five different discounted factors $\\gamma \\in \\{0, 0.1, 0.4, 0.9, 1\\}$ and perform the following tasks\n",
    "\n",
    "1. Explain the change in utilities for different $\\gamma$\n",
    "2. Explain the change in optimal policies for different $\\gamma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy iteration for ð›¾âˆˆ{0,0.1,0.4,0.9,1} :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEF_Q3 = True\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "pol_time_list = []\n",
    "for gamma in [0,0.1,0.4,0.9,1] :  \n",
    "    val2, pol2, pol_max_iter, comp_time, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta=1e-6, n_eval=n_pol_eval, plot_enable = True)\n",
    "    pol_time_list.append(comp_time)\n",
    "    plt.pause(0.1)\n",
    "DEF_Q3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value iteration for ð›¾âˆˆ{0,0.1,0.4,0.9,1} :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEF_Q3 = True\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "val_time_list = []\n",
    "for gamma in [0,0.1,0.4,0.9,1] :\n",
    "    val1, pol1, val_max_iter, comp_time, normV = value_iteration(S, A, Pr, alpha=0, discount=gamma, plot_enable=True, plot=None)\n",
    "    val_time_list.append(comp_time)\n",
    "    plt.pause(0.1)\n",
    "DEF_Q3 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0vOpHommhYZ"
   },
   "source": [
    "### IV. Plot $\\gamma$ VS computational time for the given $\\gamma$ 's in Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ð›¾ VS computational time for Policy iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Computational time [sec]')\n",
    "plt.plot([0,0.1,0.4,0.9,1], pol_time_list)\n",
    "plt.title('Discount VS Computational time for Policy iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ð›¾ VS computational time for Value iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Computational time [sec]')\n",
    "plt.plot([0,0.1,0.4,0.9,1], val_time_list)\n",
    "plt.title('Discount VS Computational time for Value iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GX-JQDtzmlqo"
   },
   "source": [
    "### V. Use five different discounted factors $\\gamma \\in \\{0, 0.1, 0.4, 0.9, 1\\}$ and preform the following tasks. Also, explain your insights.\n",
    "1. Plot $V[s]$ as a function of the iteration number in \"Policy Evaluation\" for an arbitrary state, s, in the middle of your maze. Specifically, plot $V_{k}[s]$ vs $k$.  Here, the policy is constant, as it is being evaluated. \n",
    "2. Plot $V[s]$ as a function of the iteration number in \"Policy Improvement\" for the same state as in 1. above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEF_Q5_1 = True\n",
    "\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "\n",
    "for gamma in [0,0.1,0.4,0.9,1] :  \n",
    "    \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('V_{k}[s]')\n",
    "    plt.title('V[s] as a function of the iteration number in \"Policy Evaluation\" for an arbitrary state s')\n",
    "\n",
    "    val2, pol2, pol_max_iter, time_taken, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta = 1e-6, n_eval=n_pol_eval, plot_enable = False)\n",
    "\n",
    "    plt.plot(range(len(V_ss1)), V_ss1, label = 'gamma = {}'.format(gamma))\n",
    "    plt.legend()\n",
    "\n",
    "DEF_Q5_1 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEF_Q5_2 = True\n",
    "\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "\n",
    "for gamma in [0,0.1,0.4,0.9,1] : \n",
    "    \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('V_{k}[s]')\n",
    "    plt.title('V[s] as a function of the iteration number in \"Policy Improvement\" for the same state as in above')\n",
    "    \n",
    "    val2, pol2, pol_max_iter, time_taken, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta = 1e-6, n_eval=n_pol_eval, plot_enable = False)\n",
    "    \n",
    "    plt.plot(range(len(V_ss2)), V_ss2, label = 'gamma = {}'.format(gamma))\n",
    "    plt.legend()\n",
    "\n",
    "DEF_Q5_2 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rRTBxtsEmqnk"
   },
   "source": [
    "### VI. plot norm(V_{k+1} - V_k) as a function of k, and explain your observations.\n",
    " where k is the iteration in which it converges "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $norm(V_{k+1} - V_k)$ VS $k$ for Policy iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_Q6 = True\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "val2, pol2, pol_max_iter, time_taken, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=1, theta=1e-6, n_eval=n_pol_eval, plot_enable = False)\n",
    "#print(normV)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('norm(V_{k+1} - V_k)')\n",
    "plt.ylim([0, 1000])\n",
    "plt.plot(range(len(normV)), normV)\n",
    "plt.title('norm(V_{k+1} - V_k) VS k for Policy iteration')\n",
    "DEF_Q6 = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $norm(V_{k+1} - V_k)$ VS $k$ for Value iteration :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEF_Q6 = True\n",
    "val1, pol1, val_max_iter, time_taken, normV = value_iteration(S, A, Pr, alpha=0, discount=1, plot_enable=False, plot=None)\n",
    "#print(normV)\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('norm(V_{k+1} - V_k)')\n",
    "plt.plot(range(len(normV)), normV)\n",
    "plt.title('norm(V_{k+1} - V_k) VS k for Value iteration')\n",
    "DEF_Q6 = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLbLY7ZYmvP-"
   },
   "source": [
    "### VII. Calculate the delta in iteration of the policy iteration for different theta and gamma. Plot them and also Explain your understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DEF_Q7 = True\n",
    "n_pol_eval = 100 #number of policy evalutions\n",
    "\n",
    "\n",
    "for gamma in [0.4,0.7,0.9,1] :  \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('delta')\n",
    "    plt.title('delta VS iteration for different gamma')\n",
    "    #plt.ylim([0, 1e-3])\n",
    "    val2, pol2, pol_max_iter, comp_time, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=gamma, theta=1e-6, n_eval=n_pol_eval, plot_enable=False, plot = None)\n",
    "    plt.plot(range(len(deltaV)), deltaV, label = 'gamma = {}'.format(gamma))\n",
    "    #print('*** For gamma = {}, delta = {} '.format(gamma, deltaV))\n",
    "    plt.legend()\n",
    "    \n",
    "\n",
    "for theta in [1e-1, 1e-3, 1e-6, 1e-9] :  \n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('delta')\n",
    "    plt.title('delta VS iteration for different theta')\n",
    "    val2, pol2, pol_max_iter, comp_time, normV, deltaV = policy_iteration(S, A, Pr, alpha=0, discount=1, theta=theta, n_eval=n_pol_eval, plot_enable=False, plot = None)\n",
    "    plt.plot(range(len(deltaV)), deltaV, label = 'theta = {}'.format(theta))\n",
    "    #print('*** For theta = {}, delta = {} '.format(theta, deltaV))\n",
    "    plt.legend()\n",
    "\n",
    "plt.show() \n",
    "    \n",
    "DEF_Q7 = False\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your explanations:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zo7GEfU8EUac"
   },
   "source": [
    "### B. Repeat Value and Policy iteration for 10, 20 and 100 iterations with $\\alpha \\in \\{0.2, 0.8\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGYS8OnhEUac"
   },
   "source": [
    "#### I. Policy Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3NvJ-GhEUac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx1, alpha in enumerate(ALPHA):\n",
    "    val, pol, pol_iter, time_taken, normV, deltaV = policy_iteration(S, A, Pr, alpha=alpha, discount = 1, theta=1e-6, n_eval=100, plot_enable = True, plot=[10, 20, 100])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_XEL-9LEUac"
   },
   "source": [
    "#### II. Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6r-yO1YBEUac",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for idx1, alpha in enumerate(ALPHA):\n",
    "    val, pol, val_iter, comp_time, normV = value_iteration(S, A, Pr, alpha = alpha, discount = 1, plot_enable=True, plot=[10, 20, 100])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RiVQFGDEUac"
   },
   "source": [
    "**Summarize insights and your observations in Question 1 & 2 .** "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2sLSoI6A8EAH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60btItBW7lAV"
   },
   "source": [
    "### C. Explain (up to 5 sentences) the differences between the approaches in HW1 (search, A*) and the approaches in the current assignment (MDP/Value/Policy) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
