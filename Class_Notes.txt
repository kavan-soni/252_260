Dt: 01/26/23

Human Centric
Representations of unsupervised learnings
Systems need some kind of abstraction and generalization
Transferring abilities from one task to another.

Dynamical Systems, Optimal Control, Intrinsic Motivation, Representation Learning, Empowerment2

Control systems from samples
What actions are needed for optimal dynamics when we don’t have the dynamics.
We can work from high dimensional inputs, we don’t need all specifics of data

ML and RL similarity is  -> Generalize something that is unseen

In ML, we take unseen x and predict a label y. We try to predict optimal y. The goal is instantaneous. Ground truth outputs in training
In RL, we don’t have notion of optimality. The goal in RL is to reach to goal in future. Ground Truth answer is unknown. We only know if we passed or failed( meaning we only know the reward)

Sequential decision making

Check Compass and study reference material and slides

OpenAI robot hand and cube
PPO
SAC
DDPG

Behavior Cloning - less powerful than RL
ML assumption - I.i.d - This assumption is not realistic for RL
DQN- how to improve training conditions so when we don’t have I.i.d

When we don’t have explicit extrinsic goal, agent can define goal by itself called intrinsic learning

In RL we don’t provide optimal actions, we provide reward function. Reward structure is much less restricted. If a particular actions makes our state closer to final goal, the action will have higher reward signal. Reward structure is decided by policy.

Goal in RL is an outcome of maximizing a cumulative reward


Dt: 01/31/23

Inverted Pendulum - potential embedded/ML/robotics project

Probabilistic Reasoning - General, Marginal, Discrete, Gaussian
Sequence of probabilities
Why intelligence is needed? Because there are constraints in the physical world. To reach the goal within the physical constraints, we need to distribute actions over time. Hence, sequential learning is needed
Models help us factorize/represent probabilities -> eg. Bayesian Networks
P(a0s1a1s2a2s3a3s4….|s0)
 
Types of AI problems  based on C- Control, O- Observability(if we know full state)
CO - Markov Decision Process
CO’ - Partial observable MDP - we won’t study this much in 252
C’O - Markov Chains
C’O’ - state estimation  - Hidden Markov Model, Kalman Filter - In HMM we don’t have control or observability - the dynamics are hidden
Physical devices have limitations so we may not have full state of agent/env. But we can have integrations of physical sensors.
Entropic constraint - ???
Properties of probabilities over sequences -> which probability is better, how to compare probabilities, how to quantify uncertainity
Entropy of probability, Mutual information, Distance between probability (DKL)
Relationship b/w entropy and uncertainty - > entropy is surprise factor
Entropy is measured in bits, its scalar -> try understand this in detail
If entropy is more we need more experimentations to get deterministic value of probabilities


Where do rewards come from? In human body and in RL agents
Human beings are driven by reward, we work for rewards that are so sparse in life that we may achieve them not more than once in life. Basal Ganglia
If we provide actions as input we need to provide vector that spans many features, but rewards are scalar
The complexity of formulating a vector as input is insanely complicated, however providing rewards are much sparse I.e. requires much less information and requires much less domain knowledge
Diffusion towards random goal
Sparse reward problem
Inverse Distance weighting

How to design rewards? What is a good reward? Intelligence of an agent is directly related to the design of reward system
algorithms based on Continuous vs discrete action space
Deepmind in its 1st paper published in Nature contributed wrt how to solve unavailability of iid data for RL tasks
RL is a loop of perception and action between agent and environment
Multiple Armed Bandit
Assignment 1 : representation, Variational encoders

Pertaining perception and action parts separately, end-to-end approach
Extra reading material for today — ???

Dt: 02/02/23

We cannot say where agent will be at what state at a future time so we work with probabilities

AI solutions by search methods
Heuristic based(a*) vs uniformed(bfs, dfs, Djikstra’s) search techniques

Learning is much harder when environment is fully stochastic bcoz policy don’t determine the actions

Struct search
	state space
	valid action function
	transition function
	reward function
End

Agent is policy, rest is environment
Transition is a function from spate- action space to state space  [s’ = T(s, a)]
Reward is a function from state-action-state space to a scalar  [R : X,A,X -> R']
Expected Reward = P(X'|X,A).R(X,A,X')

Understand difference between explicit and expected reward. 
We use expected reward for policy formulation bcoz the dynamics are stochastic, not deterministic.

Agent searches a sequence of actions to maximize total accumulated reward

Multiple Armed Bandit - simplest problem in RL - first problem in active learning from samples

reward distibution is different for every arm of slot machine
If you know one arm has high probability of high reward, agent just needs to keep pulling the arm
expected reward from arm1 would be more than expected reward from any other arm
Problem is only casino knows the reward distribution. An agent has to figure out which arm to pull based to its learning.
An agent has to know the expected reward. Agent maintains a table that stores the expected reward from each arm.
To efficiently use of computaion power and storage, we calculate new estimate reward from a particular arm incrementally

Since trials are limited, agent has to decide whether to keep pulling the same arm or switch to a new arm.
Greey method is to decide on only pulling the arm that has maximum expected reward at some point and disregard other arms.

Incremental Implementation : New Estimate <- Old Estimate + Stepsize[Target - OldEstimate], this is basis of Q learning
Stepsize decides how much new estimate depends on old estimate 

Upper Confidence Bound action selection
can we quantify how not be greedy and do a little exploration and decide which arm to pull.

In slot machine, states are iid because one state doesnt affect outcome of next state. Its not sequential.

Home assignement 1 : implement Multiple Armed Bandit

Dt: 02/07/23

ML@SJSU -> We meet on Fridays, 11 AM - 1 PM, ideally in-person at BBC 326 but also on Zoom.
problem is simple if you dont need very long vectors

Bayesian Networks -> multiplication of probability  factorizations. 
Once you know current state and action, now we can calculate everything, including the probability to final state. 
We want to maximze this probability to reach the final state (goal).

heuristic is estimated cost from current state to goal
Evaluation function = cost from initial to current node + cost from current node to final node
A*  -> frequently used in robotics
never overestimates the cost to reach goal and the heuristics are consistent 
meaning if we reach to another state via another intermediate state, the cost will always be higher.

Home assignment1  - > Implement A* to solve navigation in maze.

Multiple Armed Bandit Problem
Incremental implementation -> learning Q table from samples -> Q learning
seminal paper by deepmind published in nature

Variational Encoders
Represent policy as 
We need general policy to dicreetize state space.
For example, State space of inverted pendulum -> 2D -> angle of inclination, angular velocity
State factorizes past from future, we dont need past info to determine future.

Policy of action given state : ¶(A|S)
Policy is determined by probability distribution. This prob dist is parameterised by neural network

What is basic idea of auto encoding? 
We want to take images and we want to find simpler or most important vector representation of the image. (Features or latent space)
We either want to de-noise or do dimensionality reduction.
We also want to decode the representation and we want the decoded image to be as close as possible to original image.
Difference between original and decoded image is called as Loss (wrt the parameters chosen).
We want to minimise the average loss on the chosen feature set.

We want to train policy on this autoencoded features, and not on original images.
Home assignment -> calculate images samples and generate parametric represnetations. 

What is a convex function - ???
How to minimise loss function -> stochastic gradient descent 

How to find empirical covariance of data - ???

What is usual disadvantages of autoencoding?
We dont have any control on the encoded space. There could be anomalies in consistency.
Meaning similar images can be mapped far away in the representation space, ans vice versa.
It uses linear methods for vector representation. It is a deterministic method, hence inflexible.
Remark: Autoencoding in simplest cases is PCA.

We should instead use probability distrib utions in form of gaussian distribution.
x = image, z = representation, theta and psi are feature sets
encoding function f: P_theta(z|x)
decoding function g: P_psi(x|z) 

how to calculate difference between probabilities in gaussian space?
We cannot use eucledian distance in gaussian space
We use DKL function...we study this in next class.

Next class -> Variational encoders.
Next week -> 1 class with TA on sequential decision making

Dt: 02/09/23

we add a weigh factor to A* because we cannot always have admissible solution.
assignment to do : change weight factor W and observe the changes in states explored and cost of optimal path.
Tradeoff is between past and future cost. We rely more on future cost to determine next state and less on past cost.

A*, Uninformed, greedy best first search, weighted A* -> how W varies

What is meaning of admissible heuristic?

parameterized heuristics -???

What is gaussian distribution - ???

Discrete random variable -> prob function
Continous random variable -> prob density

What is inference? - Given observable data, how we can locate state with additional data
uncertaininity will alwys be lesser in latter situation.

Kalman Filter in next class.

DKL function is measure of similarity between two probabilities.

ELBO -Evident Lower Bound